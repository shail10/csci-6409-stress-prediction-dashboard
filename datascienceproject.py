# -*- coding: utf-8 -*-
"""DataScienceProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qFB5JVrIT1gqlakJpRz6NasWmRlTkjJn
"""

from google.colab import drive
drive.mount('/content/drive')

import warnings
import joblib
import os
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as mcolors
import seaborn as sns
from scipy.stats import chi2_contingency, chi2
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import f_classif
from sklearn.preprocessing import LabelEncoder

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import plot_tree, export_graphviz
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error
)
from imblearn.metrics import geometric_mean_score
import pydotplus
from IPython.display import Image

!pip install dice-ml

data = pd.read_csv("/content/drive/My Drive/PDS_Project/Mental Health Dataset.csv")
data.head()

data = data.drop(['Timestamp'], axis=1)
data.head()

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

import warnings
# Took help to write this function from Tutorial notes.
def categorical_features_report(data_df):
    def _mode(col):
        return col.mode().iloc[0] if not col.mode().empty else None

    def _mode_freq(col):
        return col.value_counts().iloc[0] if not col.value_counts().empty else 0

    def _second_mode(col):
        mode_value = _mode(col)
        second_mode = col[col != mode_value].mode()
        return second_mode.iloc[0] if not second_mode.empty else None

    def _second_mode_freq(col):
        mode_value = _mode(col)
        second_mode_value = _second_mode(col)
        if second_mode_value is not None:
            return col[col == second_mode_value].count()
        return 0

    stats = {
        "Count": lambda col: col.size,
        "Miss %": lambda col: col.isna().mean() * 100,
        "Card.": lambda col: col.nunique(),
        "Mode": _mode,
        "Mode Freq": _mode_freq,
        "Mode %": lambda col: (_mode_freq(col) / col.size * 100) if col.size > 0 else 0,
        "2nd Mode": _second_mode,
        "2nd Mode Freq": _second_mode_freq,
        "2nd Mode %": lambda col: (_second_mode_freq(col) / col.size * 100) if col.size > 0 else 0,
    }

    # Select only categorical features
    cat_feat_names = data_df.select_dtypes(exclude="number").columns
    report_data = []

    for col_name in cat_feat_names:
        col_data = data_df[col_name]
        feature_stats = {stat_name: fn(col_data) for stat_name, fn in stats.items()}
        feature_stats["Feature"] = col_name
        report_data.append(feature_stats)

    # Convert to DataFrame
    report_df = pd.DataFrame(report_data)
    return report_df

categorical_report = categorical_features_report(data)
categorical_report

"""This report gives us a brief overview of the distribution of the data. Lets dive a little deeper with Exploratory Data Analysis for the entire data.

# 1. Exploratory Data Analysis

## 1.1 Cleaning data
"""

MentalHealthDataCopy = data.copy()

# This utility function produces a report on mising values and duplicate records

def getMissingAndDuplicateRecords(df: pd.DataFrame):
    # Missing Values Calculation
    missing_values_count = df.isnull().sum()
    total_records = len(df)
    completeness = (missing_values_count / total_records) * 100
    completeness_df = pd.DataFrame({'Feature': missing_values_count.index,
                                    'Missing Values (%)': completeness.values.round(2)})

    # Duplicate Records Calculation
    duplicated_count = df.duplicated().sum()
    uniqueness_df = pd.DataFrame({
        'Total Records': [total_records],
        'Duplicate Records': [duplicated_count]
    })

    return {'Completeness Report': completeness_df, 'Uniqueness Report': uniqueness_df}

MissingAndDuplicateRecordsReport = getMissingAndDuplicateRecords(MentalHealthDataCopy)
print(MissingAndDuplicateRecordsReport['Completeness Report'])
print(MissingAndDuplicateRecordsReport['Uniqueness Report'])

"""self_employed feature has 1.78% missing values and there are 194641 duplicate records in the dataset (which can subsequently cause model to overfit)."""

# This function helps in handling missing values. Since there is only one feature with missing values, we will replace them
# with the most frequent value.

def handleMissingvalues(df):

    categorical_features = df.select_dtypes(include=['object']).columns.tolist()

    # Impute missing values for categorical features using the most frequent value
    categorical_imputer = SimpleImputer(strategy='most_frequent')
    df[categorical_features] = categorical_imputer.fit_transform(df[categorical_features])

    return df

# Removing duplicate records from the data:

def removerDuplicateRecords(df):
    df = df.drop_duplicates()
    return df

MentalHealthDataCopy = handleMissingvalues(MentalHealthDataCopy)

MentalHealthDataCopy = removerDuplicateRecords(MentalHealthDataCopy)

# Regenerating Mssing and Duplicate records report. This reveals that no missing values exist and all the records are unique.

MissingAndDuplicateRecordsReport = getMissingAndDuplicateRecords(MentalHealthDataCopy)
print(MissingAndDuplicateRecordsReport['Completeness Report'])
print(MissingAndDuplicateRecordsReport['Uniqueness Report'])

"""## 1.2 Data Visualization

This section contains various data visualization done on the categorical feature.
"""

# This function gives us a brief visualization of all the features against

plt.rcParams["figure.figsize"] = [12, 8]


def visualizeFeatures(df):
    categorical_columns = df.select_dtypes(exclude=["number"]).columns

    # Set up subplots to display multiple plots in one figure
    num_cols = len(categorical_columns)
    fig, axes = plt.subplots(nrows=(num_cols // 3) + 1, ncols=min(3, num_cols), figsize=(15, 5 * ((num_cols // 3) + 1)))
    axes = axes.flatten()

    colors = sns.color_palette("husl", num_cols)

    for i, column in enumerate(categorical_columns):
        df[column].value_counts().plot(kind='bar', color=colors[i], edgecolor='black', alpha=0.85, ax=axes[i])
        axes[i].set_title(f'Categorical Feature: {column}', fontsize=12)
        axes[i].set_xlabel(column, fontsize=10)
        axes[i].set_ylabel('Frequency', fontsize=10)
        axes[i].grid(axis='y', alpha=0.75)

    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

# Call the function on df_abt
visualizeFeatures(MentalHealthDataCopy)

"""#### Visualizing Growing stress against Gender"""

# Visualizing gender with growing stress to get more insights.

def isualizeGenderVsStress(df):
    fig, axes = plt.subplots(2, 2, figsize=(14, 9))
    sns.set_style("whitegrid")

    # Bar Chart for Gender Distribution
    gender_counts = df['Gender'].value_counts()
    axes[0, 0].barh(gender_counts.index, gender_counts.values, color=sns.color_palette("coolwarm", len(gender_counts)), edgecolor='black')
    axes[0, 0].set_xlabel('Count')
    axes[0, 0].set_title('Gender Distribution')
    for i, v in enumerate(gender_counts.values):
        axes[0, 0].text(v + 1, i, str(v), color='black', fontsize=10, fontweight='bold')

    # Donut Chart for Stress Levels
    stress_counts = df['Growing_Stress'].value_counts()
    wedges, texts, autotexts = axes[0, 1].pie(stress_counts, labels=stress_counts.index, autopct='%1.1f%%',
                                              colors=sns.color_palette("Set2", len(stress_counts)), startangle=140, pctdistance=0.85)
    center_circle = plt.Circle((0,0), 0.70, fc='white')
    axes[0, 1].add_artist(center_circle)
    axes[0, 1].set_title('Growing Stress Responses')

    # Bar Chart: Growing Stress by Gender
    df = pd.crosstab(df.Growing_Stress, df.Gender).plot(kind='bar', color=['m', 'c'], ax=axes[1, 0])
    axes[1, 0].grid(False)
    axes[1, 0].set_title('Growing Stress by Gender')
    axes[1, 0].set_xlabel("Growing Stress Levels")
    axes[1, 0].set_ylabel("Count")
    axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=0)

    axes[1, 1].axis('off')
    plt.tight_layout()
    plt.show()

isualizeGenderVsStress(MentalHealthDataCopy)

"""The data shows that most people feel more stressed these days. Women report higher stress levels than men. This means stress affects genders differently, with women experiencing it more.

#### Visualizing growing stress by days indoor
"""

def visualizeDaysIndoorVsStress(df):
    plt.figure(figsize=(10, 6))
    sns.set_style("whitegrid")

    plt.title("Growing Stress vs Days Indoors", fontsize=16)
    sns.violinplot(x="Days_Indoors", y="Growing_Stress", data=df, palette="Set2", inner="quart", linewidth=1.5)

    plt.xlabel("Days Indoors", fontsize=12)
    plt.ylabel("Growing Stress", fontsize=12)
    plt.xticks(rotation=45)
    plt.tight_layout()

    # Show plot
    plt.show()

visualizeDaysIndoorVsStress(MentalHealthDataCopy)

"""There is a relatively equal distribution across the "Days Indoors" feature. Individuals who go outside daily tend to experience lower levels of stress compared to those who stay indoors for extended periods, such as more than two months. This observation supports the notion that spending time outdoors may help reduce stress.

#### Visualizing growing stress by mental health history
"""

def visualizeMentalHealthHistoryVsStress(df):
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    sns.set_style("whitegrid")

    # Plot 1: Stacked bar chart showing stress distribution by mental health history
    cross_tab = pd.crosstab(df['Mental_Health_History'],
                           df['Growing_Stress'],
                           normalize='index') * 100

    cross_tab.plot(kind='bar', stacked=True, ax=axes[0],
                  colormap='RdYlGn_r', edgecolor='black')
    axes[0].set_title('Percentage Distribution of Stress Levels\nby Mental Health History', pad=20)
    axes[0].set_xlabel('Mental Health History', labelpad=10)
    axes[0].set_ylabel('Percentage (%)', labelpad=10)
    axes[0].legend(title='Stress Level', bbox_to_anchor=(1, 1))
    axes[0].grid(axis='y', linestyle='--', alpha=0.7)

    for n, x in enumerate([*cross_tab.index.values]):
        for (proportion, y_loc) in zip(cross_tab.loc[x], cross_tab.loc[x].cumsum()):
            if proportion > 5:  # Only show labels for segments >5% to avoid clutter
                axes[0].text(x=n, y=y_loc-proportion/2,
                           s=f'{proportion:.1f}%',
                           color='black', fontsize=9, ha='center')

    # Plot 2: Heatmap showing normalized counts
    cross_tab_counts = pd.crosstab(df['Mental_Health_History'],
                                 df['Growing_Stress'])
    sns.heatmap(cross_tab_counts, annot=True, fmt='d', cmap='YlOrRd',
               linewidths=0.5, ax=axes[1], cbar_kws={'label': 'Count'})
    axes[1].set_title('Count of Stress Levels by Mental Health History', pad=20)
    axes[1].set_xlabel('Growing Stress Level', labelpad=10)
    axes[1].set_ylabel('Mental Health History', labelpad=10)

    plt.tight_layout()
    plt.show()

visualizeMentalHealthHistoryVsStress(MentalHealthDataCopy)

"""Individuals with a prior mental health history ("Yes") report the highest stress levels (35.6%), while those with no history ("No") show moderate stress (32.7%). The "Maybe" group mirrors the "Yes" group, suggesting potential undiagnosed issues or stigma.

#### Visualizing growing stress by work interest
"""

def bubbleWorkStress(df):
    agg_data = df.groupby(['Work_Interest', 'Growing_Stress']).size().reset_index(name='Count')

    plt.figure(figsize=(15,6))
    sns.scatterplot(data=agg_data, x='Work_Interest', y='Growing_Stress',
                    size='Count', sizes=(50, 500), hue='Count',
                    palette='viridis', alpha=0.7)
    plt.title('Stress Level vs. Work Interest (Bubble Size = Frequency)', pad=20)
    plt.xlabel('Work Interest')
    plt.ylabel('Stress Level')
    plt.xticks(rotation=45)
    plt.legend(bbox_to_anchor=(1.05, 1))
    plt.grid(alpha=0.2)
    plt.tight_layout()
    plt.show()

bubbleWorkStress(MentalHealthDataCopy)

"""People highly interested in their work report the most stress, while those unsure ("Maybe") experience moderate levels. Even individuals disengaged from work feel stress, likely due to external pressures. The data suggests that passion for work amplifies stress, but avoiding work doesn’t eliminate it.

#### Growing Stress Across Different Categorical Features
"""

def plotMatrixBarplots(df, target='Growing_Stress', exclude_cols=[], plots_per_row=3):
    features = [col for col in df.columns
               if col not in [target] + exclude_cols
               and col != 'index'
               and df[col].nunique() <= 20]  # Skip high-cardinality features

    # Calculate grid size
    n_features = len(features)
    n_rows = int(np.ceil(n_features / plots_per_row))

    fig, axes = plt.subplots(n_rows, plots_per_row, figsize=(18, 5*n_rows))
    if n_rows == 1:
        axes = axes.reshape(1, -1)

    # Plot each feature
    for i, feature in enumerate(features):
        row = i // plots_per_row
        col = i % plots_per_row
        ax = axes[row, col]

        try:
            sns.countplot(x=feature, hue=target, data=df, ax=ax, palette='Set2')

            ax.set_title(f'{target} vs {feature}', fontsize=12)
            ax.set_xlabel('')
            ax.tick_params(axis='x', rotation=45)

            if col != 0:
                ax.get_legend().remove()

        except Exception as e:
            ax.set_title(f"⚠️ {feature} (Error)")
            print(f"Skipped {feature}: {str(e)}")

    for i in range(n_features, n_rows * plots_per_row):
        row = i // plots_per_row
        col = i % plots_per_row
        axes[row, col].axis('off')

    plt.tight_layout()
    plt.show()

plotMatrixBarplots(MentalHealthDataCopy, exclude_cols=['Country'])

"""# 2. Feature Selection

## 2.1 Feature Correlation Heatmap
"""

def plotCorrelationMatrix(df):

    df_copy = df.copy()
    # Encode categorical variables
    from sklearn.preprocessing import LabelEncoder
    labelencoder = LabelEncoder()
    for col in df_copy.select_dtypes(exclude=['number']).columns:
        df_copy[col] = labelencoder.fit_transform(df_copy[col])

    # Compute correlation matrix
    corr_matrix = df_copy.corr()

    # Plot heatmap
    plt.figure(figsize=(15,6))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)

    plt.title("Correlation Matrix Heatmap with Growing Stress", fontsize=16)
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.show()

plotCorrelationMatrix(MentalHealthDataCopy)

"""## 2.2 Statistical Analysis

### 2.2.1 Chi-Square Test

In this section, we have performed the Chi-Square Test. This statistical test helps us to determine if there are significant associate between two categorical feature.

**Null Hypothesis (H0): The two variables are independent.**

The test is performed between all the categorical feature and "Growing_Stress" feature, this will further help us to discard all those feature that are independent of the growing stress.
"""

# Perform Chi-Square test of independence for all categorical features against the target variable.
def performChiSquareTest(df, target_variable='Growing_Stress'):

    chi_square_results = []

    categorical_columns = df.select_dtypes(include=['object', 'category']).columns
    print(categorical_columns)
    for column in categorical_columns:
        if column != target_variable:
            # This creates a contigency table for the two features.
            crosstab = pd.crosstab(df[column], df[target_variable])

            # Perform the Chi-Square test
            stat, p, dof, expected = chi2_contingency(crosstab, correction=True)

            # Calculate the critical value based on the degrees of freedom and a significance level of 0.05
            prob = 0.95
            critical_value = chi2.ppf(prob, dof)

            # Determine if the null hypothesis is rejected (p-value < 0.05)
            reject_null = "Null Hypothesis (H0) Rejected" if p <= (1 - prob) else "Failed to reject Null Hypothesis (H0)"

            chi_square_results.append({
                'Feature': column,
                'Chi-Square Statistic': stat,
                'Degrees of Freedom': dof,
                'P-Value': p,
                'Critical Value': critical_value,
                'Conclusion': reject_null
            })

    chi_square_results_df = pd.DataFrame(chi_square_results)
    return chi_square_results_df

chiSquareTestResult = performChiSquareTest(MentalHealthDataCopy, target_variable='Growing_Stress')

chiSquareTestResult.head(20)

"""From the Chi-Square it is evident that:-

1) Country, Self-Employed Status, and care_options failed to reject the null hypothesis, implying no significant relationship with stress.

2) Days Indoors and Work interest have very high Chai-Square statistics suggesting substatial dependency with Growing_Stress.

These findings highlight the importance of lifestyle choices, mental health history, and personal habits in determining stress levels while suggesting that external factors like geographic location or employment type may not play a major role.

### 2.2.2 Mutual Information (MI)

Mutual information also quantifies the dependency between two categorical variables.

It tells us how much knowing the value of one variable reduces uncertainty about the other. Higher MI values signifies strong dependency between the features.
"""

#Returns bottom 5 features
def calculateBottom5MutualInformation(df1, target_var='Growing_Stress'):
    from sklearn.feature_selection import mutual_info_classif
    from sklearn.preprocessing import LabelEncoder
    import pandas as pd

    df_encoded_MI = df1.copy()

    # Convert categorical variables to numeric using Label Encoding
    label_encoders = {}
    for col in df_encoded_MI.columns:
        if df_encoded_MI[col].dtype == 'object':
            le = LabelEncoder()
            df_encoded_MI[col] = le.fit_transform(df_encoded_MI[col])
            label_encoders[col] = le

    X = df_encoded_MI.drop(columns=[target_var])
    y = df_encoded_MI[target_var]

    mi_scores = mutual_info_classif(X, y, discrete_features=True)

    mi_df = pd.DataFrame({'Feature': X.columns, 'MI Score': mi_scores})
    mi_df.sort_values(by="MI Score", ascending=True, inplace=True)

    return mi_df.head(5)

mi_results = calculateBottom5MutualInformation(MentalHealthDataCopy, target_var='Growing_Stress')
print(mi_results)

"""## 2.3 Feature Engineering

It is evident from the above tests that:-

1) Top Features like Work_Interest, Occupation, Changes_Habits, Days_Indoors, and Mood_Swings show strong and consistent association with the target across ANOVA, and Chi-Square. These are important and should be retained.

2) Statistical evidence supports that many features are significantly related to the target (very low p-values), even if MI scores are numerically small.

3) A few features show no meaningful relationship with the target — they have low MI, low F-values, and high Chi-Square p-values. These likely add noise and should be removed. These features involve:- **care_options, self_employed, and Country.**
"""

def dropIrrelevantFeatures(df):
    features_to_drop = ['care_options', 'self_employed', 'Country']

    df_cleaned = df.drop(columns=[col for col in features_to_drop if col in df.columns])

    return df_cleaned

MentalHealthDataCopy = dropIrrelevantFeatures(MentalHealthDataCopy)

MentalHealthDataCopy.head(10)

MentalHealthDataCopy.to_csv("MentalHealthDataCopy.csv", index=False)
from google.colab import files
files.download("MentalHealthDataCopy.csv")

"""Now we will perform encoding and get the input ready for our classifier model."""

def getEncodedData(df):

  df_encoded = df.copy()
  encoder = LabelEncoder()

  # One-Hot Encoding (OHE) for categorical features with ≤10 unique values
  low_cardinality_cols = [
    "Gender", "Occupation", "family_history", "treatment", "Days_Indoors",
    "Changes_Habits", "Mental_Health_History", "Mood_Swings", "Coping_Struggles", "Work_Interest",
    "Social_Weakness", "mental_health_interview"
  ]

  df_encoded = pd.get_dummies(df_encoded, columns=low_cardinality_cols)
  bool_cols = df_encoded.select_dtypes(include=['bool']).columns
  df_encoded[bool_cols] = df_encoded[bool_cols].astype(int)

  # Label Encoding for target variable "Growing_Stress"
  df_encoded["Growing_Stress"] = encoder.fit_transform(df_encoded["Growing_Stress"])

  return df_encoded

MentalHealthDataEncoded = getEncodedData(MentalHealthDataCopy)
MentalHealthDataEncoded.head(10)

# MentalHealthDataEncoded.to_csv("MentalHealthDataEncoded.csv", index=False)
# from google.colab import files
# files.download("MentalHealthDataEncoded.csv")

# Split the data into training (80%) and testing (20%) sets
scaler = StandardScaler()
scaled_features = scaler.fit_transform(MentalHealthDataEncoded.drop(columns=["Growing_Stress"]))
df_encoded_scaled = pd.DataFrame(scaled_features, columns=MentalHealthDataEncoded.columns[:-1])
df_encoded_scaled["Growing_Stress"] = MentalHealthDataEncoded["Growing_Stress"]

X = MentalHealthDataEncoded.drop(columns=["Growing_Stress"])
y = MentalHealthDataEncoded["Growing_Stress"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# #This code stores the scaler and X (columns) in a joblib file for future use

# if not os.path.exists("saved_models"):
#     os.makedirs("saved_models")

# joblib.dump(scaler, "saved_models/scaler.joblib")

# joblib.dump(X.columns.tolist(), "saved_models/encoded_columns.joblib")

def list_categorical_unique_values(df):
    unique_values = {}

    for col in df.select_dtypes(include=['object', 'category']).columns:
        uniques = df[col].unique().tolist()
        unique_values[col] = uniques

    return unique_values

list_categorical_unique_values(MentalHealthDataCopy)

"""# 3. Model Training

## 3.1 Model Tuning with Hyperparameters
"""

def tuneModelAndSummarize(X_train, X_test, y_train, y_test, cv=5, n_iter=40):
    model_summary = []
    bestTrainedModels = {}

    param_grids = {
        "RandomForest": {
            'n_estimators': [100, 150, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2]
        },
        "DecisionTree": {
            'max_depth': [None, 10, 20, 30],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        },
        "XGBoost": {
            'n_estimators': [100, 150],
            'max_depth': [3, 6, 10],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 1.0]
        },
        "KNN": {
            'n_neighbors': [3, 5, 7, 9],
            'weights': ['uniform', 'distance'],
            'p': [1, 2]
        },
        "LogisticRegression": {
            'C': [0.01, 0.1, 1, 10],
            'solver': ['liblinear', 'lbfgs'],
            'penalty': ['l2']
        }
    }

    base_models = {
        "RandomForest": RandomForestClassifier(random_state=42),
        "DecisionTree": DecisionTreeClassifier(random_state=42),
        "XGBoost": XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42),
        "KNN": KNeighborsClassifier(),
        "LogisticRegression": LogisticRegression(max_iter=1000, random_state=42)
    }

    for name, model in base_models.items():
        print(f"\n Tuning {name}...")
        search = RandomizedSearchCV(
            estimator=model,
            param_distributions=param_grids[name],
            n_iter=n_iter,
            cv=cv,
            n_jobs=-1,
            verbose=0,
            random_state=42
        )
        search.fit(X_train, y_train)
        best_model = search.best_estimator_
        y_pred = best_model.predict(X_test)
        test_acc = accuracy_score(y_test, y_pred)
        bestTrainedModels[name] = best_model

        print(f"\nDetailed Fold Performance for {name}:")
        fold_scores = cross_val_score(
            best_model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1
        )
        fold_scores_rounded = [round(score, 4) for score in fold_scores]
        mean_acc = round(np.mean(fold_scores), 4)
        std_acc = round(np.std(fold_scores), 4)

        model_summary.append({
            "Model": name,
            "Test Accuracy": round(test_acc, 4),
            "Best Hyperparameters": search.best_params_,
            "Fold Scores": fold_scores_rounded,  # Store list of fold scores
            "Mean CV Accuracy": mean_acc,
            "Std CV Accuracy": std_acc
        })

    model_summary_df = pd.DataFrame(model_summary)

    return model_summary_df, bestTrainedModels

[model_summary_df, bestTrainedModels] = tuneModelAndSummarize(X_train, X_test, y_train, y_test)

model_summary_df

def plot_high_accuracy_models(model_summary_df, threshold=0.9):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import pandas as pd

    # Convert fold scores into long-format DataFrame
    data = []
    for _, row in model_summary_df.iterrows():
        for score in row["Fold Scores"]:
            data.append({
                "Model": row["Model"],
                "Fold Accuracy": score
            })

    df_long = pd.DataFrame(data)

    # Filter for high-accuracy models
    high_perf_models = df_long[df_long["Fold Accuracy"] >= threshold]["Model"].unique()
    df_high = df_long[df_long["Model"].isin(high_perf_models)]

    # Plot
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df_high, x="Model", y="Fold Accuracy", palette="Set2")
    sns.stripplot(data=df_high, x="Model", y="Fold Accuracy", color="black", alpha=0.6, jitter=0.1)
    plt.ylim(df_high["Fold Accuracy"].min() - 0.002, df_high["Fold Accuracy"].max() + 0.002)
    # plt.title("Cross-Validation Accuracy (High-Accuracy Models Only)", fontsize=14)
    plt.xlabel("Model")
    plt.ylabel("Accuracy")
    plt.xticks(rotation=15)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()
plot_high_accuracy_models(model_summary_df)

# def save_models_to_joblib(models_dict, output_dir='saved_models'):
#     """
#     Saves all models in a dictionary to .joblib files.

#     """
#     if not os.path.exists(output_dir):
#         os.makedirs(output_dir)

#     for model_name, model_obj in models_dict.items():
#         filename = os.path.join(output_dir, f"{model_name}.joblib")
#         joblib.dump(model_obj, filename)
#         print(f"✅ Saved: {filename}")

# save_models_to_joblib(bestTrainedModels)
# !zip -r saved_models.zip saved_models

# from google.colab import files
# files.download('saved_models.zip')

"""## 3.2 Model Evaluation"""

def evaluate_models_metrics(models_dict, X_test, y_test):
    metrics = []

    for name, model in models_dict.items():
        y_pred = model.predict(X_test)

        # Some models (like Logistic Regression) can do probability-based AUC
        try:
            y_proba = model.predict_proba(X_test)[:, 1]
            auc = roc_auc_score(y_test, y_proba)
        except:
            auc = np.nan  # fallback if predict_proba doesn't work

        metrics.append({
            "Model": name,
            "Accuracy": round(accuracy_score(y_test, y_pred), 4),
            "MAE": round(mean_absolute_error(y_test, y_pred), 4),
            "MSE": round(mean_squared_error(y_test, y_pred), 4),
            "RMSE": round(np.sqrt(mean_squared_error(y_test, y_pred)), 4),
            "Precision": round(precision_score(y_test, y_pred, average='weighted'), 4),
            "Recall": round(recall_score(y_test, y_pred, average='weighted'), 4),
            "F1 Score": round(f1_score(y_test, y_pred, average='weighted'), 4),
            "Geometric Mean": round(geometric_mean_score(y_test, y_pred), 4),
            "AUC Score": round(auc, 4) if not np.isnan(auc) else "N/A"
        })

    return pd.DataFrame(metrics)

evaluate_models_metrics(bestTrainedModels,X_test,y_test)

# Function to check overfitting
def check_overfitting(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)  # Train the model

    train_acc = accuracy_score(y_train, model.predict(X_train))  # Training accuracy
    test_acc = accuracy_score(y_test, model.predict(X_test))  # Testing accuracy

    print(f"{model.__class__.__name__}:")
    print(f"Training Accuracy: {train_acc:.4f}")
    print(f"Testing Accuracy:  {test_acc:.4f}")
    print(f"Overfitting Gap:   {train_acc - test_acc:.4f}\n")  # Gap should be small

# Models to check

models = {
    "XGBoost": XGBClassifier(eval_metric="logloss"),
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "KNN": KNeighborsClassifier()
}

for name, model in models.items():
    check_overfitting(model, X_train, X_test, y_train, y_test)

"""## 3.3 Learning Curves"""

from sklearn.model_selection import LearningCurveDisplay, learning_curve

def plot_learning_curves(models_dict, X_train, y_train):

    # Filter only the models we need
    selected_models = ["RandomForest", "KNN", "XGBoost", "DecisionTree"]
    models_to_plot = {name: model for name, model in models_dict.items() if name in selected_models}

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 layout

    for ax, (name, model) in zip(axes.ravel(), models_to_plot.items()):
        train_sizes, train_scores, test_scores = learning_curve(
            model, X_train, y_train, cv=5, scoring="accuracy",
            n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)
        )

        # Use LearningCurveDisplay
        display = LearningCurveDisplay(train_sizes=train_sizes, train_scores=train_scores, test_scores=test_scores)
        display.plot(ax=ax)

        ax.set_title(f"Learning Curve - {name}")
        ax.set_xlabel("Training Examples")
        ax.set_ylabel("Accuracy")

    plt.tight_layout()
    plt.show()

plot_learning_curves(bestTrainedModels, X_train, y_train)

"""## 3.4 Feature Importance"""

def get_feature_importance_matrix(models_dict, feature_names):
    import pandas as pd
    import re

    importance_data = []

    for model_name, model in models_dict.items():
        # Get importances
        if hasattr(model, "feature_importances_"):
            importances = model.feature_importances_
        elif hasattr(model, "coef_"):
            importances = model.coef_
            if importances.ndim > 1:
                importances = abs(importances[0])
            else:
                importances = abs(importances)
        else:
            print(f"⚠️ Skipping {model_name} — no feature importance attribute.")
            continue

        df = pd.DataFrame({
            "Encoded Feature": feature_names,
            "Importance": importances
        })

        # Extracting base feature name by removing last underscore part
        df["Original Feature"] = df["Encoded Feature"].apply(lambda x: re.split(r'_(?!.*_)', x)[0])

        # Group importance by original feature name
        grouped = df.groupby("Original Feature")["Importance"].sum().reset_index()
        grouped.columns = ["Feature", model_name]

        importance_data.append(grouped)

    # Merge all importance DataFrames on "Feature"
    importance_matrix = importance_data[0]
    for df in importance_data[1:]:
        importance_matrix = pd.merge(importance_matrix, df, on="Feature", how="outer")

    # Fill NaNs with 0 for models that skipped a feature
    importance_matrix.fillna(0, inplace=True)

    importance_matrix.set_index("Feature", inplace=True)

    return importance_matrix


importance_matrix_df = get_feature_importance_matrix(bestTrainedModels, X_train.columns)

importance_matrix_df.head(100)

def plot_feature_importance(importance_matrix_df):
    models = importance_matrix_df.columns.tolist()
    features = importance_matrix_df.index.tolist()

    x = np.arange(len(features))  # the label locations
    width = 0.2  # the width of the bars
    colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightsalmon']

    fig, ax = plt.subplots(figsize=(14, 8))

    for i, (model, color) in enumerate(zip(models, colors)):
        bars = ax.bar(x + i * width, importance_matrix_df[model], width, label=model, color=color)

        for bar in bars:
            height = bar.get_height()
            ax.text(
                bar.get_x() + bar.get_width() / 2, height + 0.01,  # Position above the bar
                f'{height:.4f}', ha='center', va='bottom', fontsize=9, rotation=90, color='black'
            )

    ax.set_xlabel("Features")
    ax.set_ylabel("Importance Values")
    ax.set_title("Feature Importance Across Models")
    ax.set_xticks(x + width * (len(models) / 2 - 0.5))
    ax.set_xticklabels(features, rotation=45)
    ax.legend()

    ax.set_yticks(np.arange(0, max(importance_matrix_df.max()) + 0.10, 0.05))

    plt.tight_layout()
    plt.show()

plot_feature_importance(importance_matrix_df)

"""## 3.5 Counterfactual Analysis

Features importance tell us the importance of feature with respect to each model. But it doesn't give us an insight on how this feature can influence our result. For that we have performed a Counterfactual Analysis on our two best models, Random Forest and Decision Trees.

Counterfactual Analysis gives us an answer to the question:-

“What minimal change would have caused a different outcome?”

It means identifying how an input instance could be slightly altered to change the model's prediction.

The below function multiclass_counterfactual_analysis_all_models performs model level counterfactual analysis for the given multiclass classification using the DICE library. For the two models (Random Forest and Decision Tree), it generates counterfactual examples of a subset of a test instances and indentifies which features most frequently needed to change to alter model's prediction.
"""

!pip install dice-ml

import dice_ml
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

def multiclass_counterfactual_analysis_all_models(models_dict, dataset, X_test, num_samples=50, cfs_per_instance=2, target_col='Growing_Stress'):
    """
    Generates model-level counterfactual analysis for all models in models_dict.

    Args:
        num_samples: Number of test samples to use.
        cfs_per_instance: Counterfactuals generated per desired class.
        target_col: Target column name.

    Returns:
        Dictionary of DataFrames containing counterfactual feature summaries per model.
    """
    categorical_features = [col for col in X_test.columns]

    results_summary = {}

    for model_name, model in models_dict.items():
        print(f"\nAnalyzing Model: {model_name}\n{'-'*40}")

        dice_data = dice_ml.Data(
            dataframe=dataset,
            continuous_features=[],
            categorical_features=categorical_features,
            outcome_name=target_col
        )

        dice_model = dice_ml.Model(model=model, backend="sklearn")
        explainer = dice_ml.Dice(dice_data, dice_model, method="random")

        feature_change_counts = {feature: 0 for feature in X_test.columns}
        samples_used = min(num_samples, len(X_test))

        selected_indices = np.random.choice(X_test.index, samples_used, replace=False)

        for idx in selected_indices:
            instance = X_test.loc[idx]
            current_pred = model.predict(instance.to_frame().T)[0]

            possible_classes = [0,1,2]
            desired_classes = [cls for cls in possible_classes if cls != current_pred]

            for desired_class in desired_classes:
                counterfactuals = explainer.generate_counterfactuals(
                    instance.to_frame().T,
                    total_CFs=cfs_per_instance,
                    desired_class=desired_class,
                    verbose=False
                )

                cf_df = counterfactuals.cf_examples_list[0].final_cfs_df.drop(columns=[target_col])

                for _, cf_instance in cf_df.iterrows():
                    for feature in X_test.columns:
                        if cf_instance[feature] != instance[feature]:
                            feature_change_counts[feature] += 1

        feature_change_summary = pd.DataFrame.from_dict(feature_change_counts, orient='index', columns=['change_count'])
        total_possible_changes = samples_used * cfs_per_instance * 2  # two alternative classes per instance
        feature_change_summary['change_frequency'] = feature_change_summary['change_count'] / total_possible_changes
        feature_change_summary.sort_values('change_frequency', ascending=False, inplace=True)

        print(feature_change_summary)

        results_summary[model_name] = feature_change_summary

    return results_summary


selected_models = {name: model for name, model in bestTrainedModels.items()
                   if 'randomforest' in name.lower() or 'decisiontree' in name.lower()}

feature_summaries = multiclass_counterfactual_analysis_all_models(
    models_dict=selected_models,
    dataset=MentalHealthDataEncoded,
    X_test=X_test,
    num_samples=2000,
    cfs_per_instance=2,
    target_col='Growing_Stress'
)

def aggregate_feature_summaries(feature_summaries):
    """
    Aggregates one-hot encoded feature names into original categorical features
    across all models in feature_summaries.

    Args:
        feature_summaries: dict of {model_name: feature_importance_df}

    Returns:
        dict of {model_name: aggregated_feature_importance_df}
    """
    aggregated_dict = {}

    for model_name, summary_df in feature_summaries.items():
        aggregated = {}

        for feature in summary_df.index:
            # Try splitting only the last part after last underscore
            if '_' in feature:
                base = '_'.join(feature.split('_')[:-1])
            else:
                base = feature

            if base in aggregated:
                aggregated[base] += summary_df.loc[feature, 'change_count']
            else:
                aggregated[base] = summary_df.loc[feature, 'change_count']

        # Convert to DataFrame
        agg_df = pd.DataFrame.from_dict(aggregated, orient='index', columns=['aggregated_change_count'])
        total = agg_df['aggregated_change_count'].sum()
        agg_df['aggregated_change_frequency'] = agg_df['aggregated_change_count'] / total
        agg_df.sort_values('aggregated_change_frequency', ascending=False, inplace=True)

        aggregated_dict[model_name] = agg_df

    return aggregated_dict

aggregated_feature_summaries = aggregate_feature_summaries(feature_summaries)

aggregated_feature_summaries['RandomForest']

aggregated_feature_summaries['DecisionTree']

"""Aggregated Change Count: How many times this feature had to change in counterfactual examples to flip the model's prediction.

Aggregated Change Frequency: How frequently this feature was part of the minimal change set (i.e., how “important” it is for influencing prediction decisions).

Interpretation:-

1) Occupation (17%) and Days_Indoors (17.2%) are the top features your model relies on to predict growing stress.
→ Changing a person's occupation or how long they stay indoors often flips the stress prediction.

2) Mood_Swings, Changes_Habits, and Work_Interest are also major influencers — intuitively aligned with behavioral patterns associated with stress.

3) treatment had 0 impact on prediction flips.
→ Either the model doesn’t consider it useful, or it’s not varying enough in the data to influence outcomes.
"""

import matplotlib.pyplot as plt
import seaborn as sns

def plot_feature_change_summary(summary_df, model_name):
    # Sort by frequency for clean plotting
    sorted_df = summary_df.sort_values('aggregated_change_frequency', ascending=True)

    plt.figure(figsize=(10, 6))
    ax = sns.barplot(
        x='aggregated_change_frequency',
        y=sorted_df.index,
        data=sorted_df,
        palette='viridis'
    )

    # Add count labels to bars
    for i, (count, freq) in enumerate(zip(sorted_df['aggregated_change_count'], sorted_df['aggregated_change_frequency'])):
        ax.text(freq + 0.002, i, f'{count}', va='center', fontsize=9)

    plt.title(f'Feature Change Frequency with Counts - {model_name} (2000 Samples)', fontsize=14)
    plt.xlabel('Aggregated Change Frequency')
    plt.ylabel('Features')
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.show()

plot_feature_change_summary(aggregated_feature_summaries['RandomForest'], model_name='Random Forest')

plot_feature_change_summary(aggregated_feature_summaries['DecisionTree'], model_name='Decision Tree')